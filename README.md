#  Cloud Travel Data ETL Pipeline (Python + PostgreSQL)

##  Professional Context

This project was built as part of my transition back into Data Engineering, leveraging my prior experience in SQL database administration and Azure SQL cloud support.  

It demonstrates production-style ETL design principles using Python and a cloud-hosted PostgreSQL data warehouse (Neon).

---

##  Project Overview

This project implements an end-to-end ETL (Extract, Transform, Load) pipeline that:

- Ingests raw travel booking data from CSV files  
- Validates and cleans the data  
- Saves processed data for auditability  
- Loads structured data into Neon PostgreSQL  
- Implements logging and idempotent database inserts  

The goal is to simulate a real-world data engineering workflow using modular architecture and cloud infrastructure.

---

##  Architecture

Raw CSV â†’ Data Validation and cleaning â†’ Transformation- Currency Standardization (EUR, GBP â†’ USD) â†’ Processed CSV â†’ PostgreSQL (Neon Cloud)

```
data/raw/
    â†“
src/transform/
    â†“
data/processed/
    â†“
src/load/
    â†“
Neon PostgreSQL
    â†“  
Generate Analytical Reports
```

---

##  Tech Stack

- Python  
- Pandas
- SQL 
- PostgreSQL (Neon Cloud)   
- YAML configuration management  
- Git & GitHub  
- Python logging module  

---

##  Pipeline Features

âœ” Modular project structure (ingest / transform / load / utils) 
âœ” CSV ingestion (supports large datasets â€“ 200+ records tested)
âœ” Data validation checks  
âœ” Duplicate removal  
âœ” Standardized formatting- Currency standardization to USD 
âœ” Idempotent inserts using PostgreSQL `ON CONFLICT`  
âœ” Timestamped logging for monitoring  
âœ” Automated analytical report generation
âœ” Separation of credentials using config.yaml 


---

##  Project Structure

```
travel-data-pipeline/
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/                # Raw input datasets
â”‚   â”œâ”€â”€ processed/          # Cleaned data files
â”‚
â”œâ”€â”€ logs/                   # ETL execution logs
â”‚
â”œâ”€â”€ portfolio_outputs/      # Analytical output summaries
â”‚
â”œâ”€â”€ sql/
â”‚   â”œâ”€â”€ schema/             # Table creation scripts
â”‚   â”œâ”€â”€ models/             # Analytical SQL queries
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ ingest/             # ETL ingestion logic
â”‚   â”œâ”€â”€ transform/          # Data cleaning & normalization
â”‚   â”œâ”€â”€ load/               # Reporting & aggregation scripts
â”‚   â”œâ”€â”€ utils/              # Database connection utilities
â”‚
â”œâ”€â”€ generate_csv.py         # Script to generate large sample dataset
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

---
##  Database Schema

```sql
CREATE TABLE IF NOT EXISTS travel_bookings (
    booking_id INT PRIMARY KEY,
    user_id INT,
    destination VARCHAR(100),
    booking_date DATE,
    travel_date DATE,
    price NUMERIC,
    currency VARCHAR(10),
    booking_status VARCHAR(20)
);
```

---

##  Example Analytical Reports Generated

- Total Revenue  
- Revenue by Destination  
- Bookings per Month  
- Top Destinations by Booking Volume  
- Booking Status Distribution  

Reports are saved in:

portfolio_outputs/

---
##  How to Run

pip install -r requirements.txt

Activate virtual environment:

```
venv\Scripts\activate
```
To Configure database connection

Create a file:
src/utils/config.py

Add your Neon credentials:

```python
DB_HOST = "your_neon_host"
DB_NAME = "your_database"
DB_USER = "your_username"
DB_PASSWORD = "your_password"
```

To run the ETL pipeline:

```
python -m src.ingest.ingest_csv_to_postgres
```
To Generate Analytical reports:

```
python -m src.load.load_summary
```
---

##  Example Queries

```sql
SELECT COUNT(*) FROM travel_bookings;

SELECT * 
FROM travel_bookings 
WHERE booking_status = 'confirmed';
```

---

##  What This Demonstrates

- Cloud database integration
- End-to-end Data pipeline design
- SQL schema creation
- Data transformation using Pandas
- Logging & monitoring
- Production-style project structuring
- Analytical reporting automation  
- Version control using Git

---

##  Future Enhancements

- Airflow orchestration
- AWS S3 integration
- Data quality framework
- CI/CD automation

---

## ğŸ‘©â€ğŸ’» Author

**Akshatha B**  
SQL | Cloud | Data Engineering  

Open to Data Engineering opportunities.

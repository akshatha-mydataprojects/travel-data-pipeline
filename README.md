#  End-to-End ETL Pipeline for Travel Booking Data (Python + PostgreSQL)

##  Professional Context

This project was built as part of my transition back into Data Engineering, leveraging my prior experience in SQL database administration and Azure SQL cloud support.  

It simulates a real-world use case for a global online travel booking company and demonstrates production-style ETL design using Python and a cloud-hosted PostgreSQL data warehouse (Neon).

---

##  Business Scenario

The company receives daily booking data exports from multiple regions, in different currencies (USD, EUR, GBP), and lacks centralized reporting. Finance struggles to calculate total revenue in USD, and leadership cannot track destination performance or monthly booking trends reliably.

###  Objective

The objective is to:

- Centralize raw booking data into a cloud data warehouse  
- Standardize all prices into USD  
- Validate and clean incoming datasets  
- Enable automated analytical reporting  
- Provide business-ready insights such as revenue by destination and monthly booking trends  

##  How the Pipeline Solves It

The pipeline addresses the business challenges as follows:

- **Centralized Data**: Ingests raw CSVs from multiple regions into a single Neon PostgreSQL warehouse  
- **Data Validation & Cleaning**: Removes duplicates, ensures correct date formats, and checks for missing critical values  
- **Currency Standardization**: Converts EUR and GBP to USD for consistent revenue calculations  
- **Automated Reporting**: Generates analytical summaries such as revenue per destination, monthly bookings, top destinations, and booking status distribution  
- **Logging & Monitoring**: Tracks ETL execution to ensure reliability and reproducibility  

This allows business stakeholders to make **data-driven decisions**, monitor **destination performance**, and accurately **track revenue trends** in USD.

---
##  Project Overview

This project implements an end-to-end ETL (Extract, Transform, Load) pipeline that:

- Ingests raw travel booking data from CSV files  
- Validates and cleans the data  
- Saves processed data for auditability  
- Loads structured data into Neon PostgreSQL  
- Implements logging and idempotent database inserts
- Generates Analytical reports  

---
##  Architecture

Raw CSV â†’ Data Validation â†’ Cleaning and Transformation â†’ Processed CSV â†’ PostgreSQL (Neon Cloud) â†’ Generate Analytical Reports

```
data/raw/
    â†“
src/transform/
    â†“
data/processed/
    â†“
src/load/
    â†“
Neon PostgreSQL
    â†“  
Generate Analytical Reports
```

---

##  Tech Stack

- Python  
- Pandas
- SQL 
- PostgreSQL (Neon Cloud)   
- YAML configuration management    
- Python logging module
- Git & GitHub 

---

##  Pipeline Features

- Modular project structure (ingest / transform / load / utils) 
- CSV ingestion (supports large datasets)
- Data validation checks  
- Duplicate removal  
- Standardized formatting- Currency Standardization (EUR, GBP â†’ USD) 
- Idempotent inserts using PostgreSQL `ON CONFLICT`  
- Timestamped logging for monitoring  
- Automated analytical report generation
- Separation of credentials using config.yaml 

---

##  Project Structure

```
travel-data-pipeline/
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/                # Raw input datasets
â”‚   â”œâ”€â”€ processed/          # Cleaned data files
â”‚
â”œâ”€â”€ logs/                   # ETL execution logs
â”‚
â”œâ”€â”€ portfolio_outputs/      # Analytical output summaries
â”‚
â”œâ”€â”€ sql/
â”‚   â”œâ”€â”€ schema/             # Table creation scripts
â”‚   â”œâ”€â”€ models/             # Analytical SQL queries
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ ingest/             # ETL ingestion logic
â”‚   â”œâ”€â”€ transform/          # Data cleaning & normalization
â”‚   â”œâ”€â”€ load/               # Reporting & aggregation scripts
â”‚   â”œâ”€â”€ utils/              # Database connection utilities
â”‚
â”œâ”€â”€ generate_csv.py         # Script to generate large sample dataset
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

---
##  Database Schema

```sql
CREATE TABLE IF NOT EXISTS travel_bookings (
    booking_id INT PRIMARY KEY,
    user_id INT,
    destination VARCHAR(100),
    booking_date DATE,
    travel_date DATE,
    price NUMERIC,
    currency VARCHAR(10),
    booking_status VARCHAR(20)
);
```

---

##  Example Analytical Reports Generated

- Total Revenue  
- Revenue by Destination  
- Bookings per Month  
- Top Destinations by Booking Volume  
- Booking Status Distribution  

Reports are saved in:

portfolio_outputs/

---
##  How to Run

pip install -r requirements.txt

Activate virtual environment:

```
venv\Scripts\activate
```
To Configure database connection:

Create a file:

src/utils/config.py

Add your Neon credentials:

```python
DB_HOST = "your_neon_host"
DB_NAME = "your_database"
DB_USER = "your_username"
DB_PASSWORD = "your_password"
```

To run the ETL pipeline:

```
python -m src.ingest.ingest_csv_to_postgres
```
To Generate Analytical reports:

```
python -m src.load.load_summary
```
---

##  Example Queries

```sql
SELECT COUNT(*) FROM travel_bookings;

SELECT destination, COUNT(*) AS booking_count
FROM travel_bookings
GROUP BY destination
ORDER BY booking_count DESC
LIMIT 5;

```

---


##  Future Enhancements

- Airflow orchestration
- AWS S3 integration
- Data quality framework
- CI/CD automation

---

## ğŸ‘©â€ğŸ’» Author

**Akshatha B**  
SQL | Cloud | Data Engineering  

Open to Data Engineering opportunities.
